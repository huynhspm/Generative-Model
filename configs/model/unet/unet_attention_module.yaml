_target_: src.models.unet.UNetModule

use_ema: true

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.005
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ExponentialLR
  _partial_: true
  gamma: 0.95

net:
  _target_: src.models.unet.net.UNetAttention

  in_channels: 1 # the channel count of the input
  out_channels: 1 # the channel count of the output
  base_channels: 64 # the base channel count for the model
  block: Residual # type of blocks for each level
  n_layer_blocks: 1 # number of blocks at each level
  channel_multipliers: [1, 2, 4] # the multiplicative factors for number of channels for each level
  attention: SelfAttention # type of attentions for each level
  attention_levels: [1, 2] # the levels at which attention should be performed
  n_attention_heads: 4 # the number of attention heads
  n_attention_layers: 1 # the number of attention layers

criterion:
  # Dice Loss
  _target_: segmentation_models_pytorch.losses.DiceLoss
  mode: binary
  log_loss: true
  from_logits: true
